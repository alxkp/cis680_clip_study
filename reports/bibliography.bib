
@misc{kangCLIPIdealNo2025,
	title = {Is {CLIP} ideal? No. Can we fix it? Yes!},
	url = {http://arxiv.org/abs/2503.08723},
	doi = {10.48550/arXiv.2503.08723},
	shorttitle = {Is {CLIP} ideal?},
	abstract = {Contrastive Language-Image Pre-Training ({CLIP}) is a popular method for learning multimodal latent spaces with well-organized semantics. Despite its wide range of applications, {CLIP}'s latent space is known to fail at handling complex visual-textual interactions. Recent works attempt to address its shortcomings with data-centric or algorithmic approaches. But what if the problem is more fundamental, and lies in the geometry of {CLIP}? Toward this end, we rigorously analyze {CLIP}'s latent space properties, and prove that no {CLIP}-like joint embedding space exists which can correctly do any two of the following at the same time: 1. represent basic descriptions and image content, 2. represent attribute binding, 3. represent spatial location and relationships, 4. represent negation. Informed by this analysis, we propose Dense Cosine Similarity Maps ({DCSMs}) as a principled and interpretable scoring method for {CLIP}-like models, which solves the fundamental limitations of {CLIP} by retaining the semantic topology of the image patches and text tokens. This method improves upon the performance of classical {CLIP}-like joint encoder models on a wide array of benchmarks. We share our code and data here for reproducibility: https://github.com/Raphoo/{DCSM}\_Ideal\_CLIP},
	number = {{arXiv}:2503.08723},
	publisher = {{arXiv}},
	author = {Kang, Raphi and Song, Yue and Gkioxari, Georgia and Perona, Pietro},
	urldate = {2025-10-14},
	date = {2025-03-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2503.08723 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {PDF:/Users/alxkp/Zotero/storage/BCSZMDZG/Kang et al. - 2025 - Is CLIP ideal No. Can we fix it Yes!.pdf:application/pdf},
}

@misc{yangAlignedCutVisualConcepts2024,
	title = {{AlignedCut}: Visual Concepts Discovery on Brain-Guided Universal Feature Space},
	url = {http://arxiv.org/abs/2406.18344},
	doi = {10.48550/arXiv.2406.18344},
	shorttitle = {{AlignedCut}},
	abstract = {We study the intriguing connection between visual data, deep networks, and the brain. Our method creates a universal channel alignment by using brain voxel {fMRI} response prediction as the training objective. We discover that deep networks, trained with different objectives, share common feature channels across various models. These channels can be clustered into recurring sets, corresponding to distinct brain regions, indicating the formation of visual concepts. Tracing the clusters of channel responses onto the images, we see semantically meaningful object segments emerge, even without any supervised decoder. Furthermore, the universal feature alignment and the clustering of channels produce a picture and quantification of how visual information is processed through the different network layers, which produces precise comparisons between the networks.},
	number = {{arXiv}:2406.18344},
	publisher = {{arXiv}},
	author = {Yang, Huzheng and Gee, James and Shi, Jianbo},
	urldate = {2025-10-24},
	date = {2024-06-26},
	eprinttype = {arxiv},
	eprint = {2406.18344 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/alxkp/Zotero/storage/HXNW3Q9S/Yang et al. - 2024 - AlignedCut Visual Concepts Discovery on Brain-Guided Universal Feature Space.pdf:application/pdf;Snapshot:/Users/alxkp/Zotero/storage/3S9B28GU/2406.html:text/html},
}

@misc{radfordLearningTransferableVisual2021,
	title = {Learning Transferable Visual Models From Natural Language Supervision},
	url = {http://arxiv.org/abs/2103.00020},
	doi = {10.48550/arXiv.2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn {SOTA} image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as {OCR}, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original {ResNet}-50 on {ImageNet} zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/{OpenAI}/{CLIP}.},
	number = {{arXiv}:2103.00020},
	publisher = {{arXiv}},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	urldate = {2025-11-06},
	date = {2021-02-26},
	eprinttype = {arxiv},
	eprint = {2103.00020 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/alxkp/Zotero/storage/Y7SBTSDT/Radford et al. - 2021 - Learning Transferable Visual Models From Natural Language Supervision.pdf:application/pdf;Snapshot:/Users/alxkp/Zotero/storage/2434DHTJ/2103.html:text/html},
}

@article{shiNormalizedCutsImage2000,
	title = {Normalized cuts and image segmentation},
	volume = {22},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/document/868688},
	doi = {10.1109/34.868688},
	abstract = {We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We applied this approach to segmenting static images, as well as motion sequences, and found the results to be very encouraging.},
	pages = {888--905},
	number = {8},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Shi, Jianbo and Malik, J.},
	urldate = {2025-11-06},
	date = {2000-08},
	keywords = {Bayesian methods, Brightness, Clustering algorithms, Coherence, Data mining, Eigenvalues and eigenfunctions, Filling, Image segmentation, Partitioning algorithms, Tree data structures},
	file = {Full Text PDF:/Users/alxkp/Zotero/storage/LRT3Z225/Shi and Malik - 2000 - Normalized cuts and image segmentation.pdf:application/pdf},
}

@article{amariNaturalGradientWorks1998,
	title = {Natural Gradient Works Efficiently in Learning},
	volume = {10},
	issn = {0899-7667},
	url = {https://ieeexplore.ieee.org/document/6790500},
	doi = {10.1162/089976698300017746},
	abstract = {When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed.},
	pages = {251--276},
	number = {2},
	journaltitle = {Neural Computation},
	author = {Amari, Shun-ichi},
	urldate = {2025-11-06},
	date = {1998-02},
	file = {Snapshot:/Users/alxkp/Zotero/storage/M4BFRZ4J/6790500.html:text/html},
}

@misc{oordNeuralDiscreteRepresentation2018,
	title = {Neural Discrete Representation Learning},
	url = {http://arxiv.org/abs/1711.00937},
	doi = {10.48550/arXiv.1711.00937},
	abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector {QuantisedVariational} {AutoEncoder} ({VQ}-{VAE}), differs from {VAEs} in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation ({VQ}). Using the {VQ} method allows the model to circumvent issues of “posterior collapse” -— where the latents are ignored when they are paired with a powerful autoregressive decoder -— typically observed in the {VAE} framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
	number = {{arXiv}:1711.00937},
	publisher = {{arXiv}},
	author = {Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
	urldate = {2025-11-06},
	date = {2018-05-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1711.00937 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {PDF:/Users/alxkp/Zotero/storage/LSRRAWTA/Oord et al. - 2018 - Neural Discrete Representation Learning.pdf:application/pdf},
}

@misc{gandelsmanInterpretingSecondOrderEffects2025,
	title = {Interpreting the Second-Order Effects of Neurons in {CLIP}},
	url = {http://arxiv.org/abs/2406.04341},
	doi = {10.48550/arXiv.2406.04341},
	abstract = {We interpret the function of individual neurons in {CLIP} by automatically describing them using text. Analyzing the direct effects (i.e. the flow from a neuron through the residual stream to the output) or the indirect effects (overall contribution) fails to capture the neurons' function in {CLIP}. Therefore, we present the "second-order lens", analyzing the effect flowing from a neuron through the later attention heads, directly to the output. We find that these effects are highly selective: for each neuron, the effect is significant for {\textless}2\% of the images. Moreover, each effect can be approximated by a single direction in the text-image space of {CLIP}. We describe neurons by decomposing these directions into sparse sets of text representations. The sets reveal polysemantic behavior - each neuron corresponds to multiple, often unrelated, concepts (e.g. ships and cars). Exploiting this neuron polysemy, we mass-produce "semantic" adversarial examples by generating images with concepts spuriously correlated to the incorrect class. Additionally, we use the second-order effects for zero-shot segmentation, outperforming previous methods. Our results indicate that an automated interpretation of neurons can be used for model deception and for introducing new model capabilities.},
	number = {{arXiv}:2406.04341},
	publisher = {{arXiv}},
	author = {Gandelsman, Yossi and Efros, Alexei A. and Steinhardt, Jacob},
	urldate = {2025-12-18},
	date = {2025-02-12},
	eprinttype = {arxiv},
	eprint = {2406.04341 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/alxkp/Zotero/storage/IDKZRCBT/Gandelsman et al. - 2025 - Interpreting the Second-Order Effects of Neurons in CLIP.pdf:application/pdf;Snapshot:/Users/alxkp/Zotero/storage/7TAPMX2H/2406.html:text/html},
}

@misc{thrushWinogroundProbingVision2022,
	title = {Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality},
	url = {http://arxiv.org/abs/2204.03162},
	doi = {10.48550/arXiv.2204.03162},
	shorttitle = {Winoground},
	abstract = {We present a novel task and dataset for evaluating the ability of vision and language models to conduct visio-linguistic compositional reasoning, which we call Winoground. Given two images and two captions, the goal is to match them correctly - but crucially, both captions contain a completely identical set of words, only in a different order. The dataset was carefully hand-curated by expert annotators and is labeled with a rich set of fine-grained tags to assist in analyzing model performance. We probe a diverse range of state-of-the-art vision and language models and find that, surprisingly, none of them do much better than chance. Evidently, these models are not as skilled at visio-linguistic compositional reasoning as we might have hoped. We perform an extensive analysis to obtain insights into how future work might try to mitigate these models' shortcomings. We aim for Winoground to serve as a useful evaluation set for advancing the state of the art and driving further progress in the field. The dataset is available at https://huggingface.co/datasets/facebook/winoground.},
	number = {{arXiv}:2204.03162},
	publisher = {{arXiv}},
	author = {Thrush, Tristan and Jiang, Ryan and Bartolo, Max and Singh, Amanpreet and Williams, Adina and Kiela, Douwe and Ross, Candace},
	urldate = {2025-12-18},
	date = {2022-04-22},
	eprinttype = {arxiv},
	eprint = {2204.03162 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/alxkp/Zotero/storage/LR82KEK9/Thrush et al. - 2022 - Winoground Probing Vision and Language Models for Visio-Linguistic Compositionality.pdf:application/pdf;Snapshot:/Users/alxkp/Zotero/storage/NWQJQMH7/2204.html:text/html},
}

@misc{tishbyDeepLearningInformation2015,
	title = {Deep Learning and the Information Bottleneck Principle},
	url = {http://arxiv.org/abs/1503.02406},
	doi = {10.48550/arXiv.1503.02406},
	abstract = {Deep Neural Networks ({DNNs}) are analyzed via the theoretical framework of the information bottleneck ({IB}) principle. We first show that any {DNN} can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the {DNN} and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
	number = {{arXiv}:1503.02406},
	publisher = {{arXiv}},
	author = {Tishby, Naftali and Zaslavsky, Noga},
	urldate = {2025-12-18},
	date = {2015-03-09},
	eprinttype = {arxiv},
	eprint = {1503.02406 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/alxkp/Zotero/storage/SNMBXYQT/Tishby and Zaslavsky - 2015 - Deep Learning and the Information Bottleneck Principle.pdf:application/pdf},
}

@misc{zhaiSigmoidLossLanguage2023,
	title = {Sigmoid Loss for Language Image Pre-Training},
	url = {http://arxiv.org/abs/2303.15343},
	doi = {10.48550/arXiv.2303.15343},
	abstract = {We propose a simple pairwise Sigmoid loss for Language-Image Pre-training ({SigLIP}). Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. Combined with Locked-image Tuning, with only four {TPUv}4 chips, we train a {SigLiT} model that achieves 84.5\% {ImageNet} zero-shot accuracy in two days. The disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient. We release our models at https://github.com/google-research/big\_vision and hope our research motivates further explorations in improving the quality and efficiency of language-image pre-training.},
	number = {{arXiv}:2303.15343},
	publisher = {{arXiv}},
	author = {Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
	urldate = {2025-12-18},
	date = {2023-09-27},
	eprinttype = {arxiv},
	eprint = {2303.15343 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/alxkp/Zotero/storage/XLJS5SW2/Zhai et al. - 2023 - Sigmoid Loss for Language Image Pre-Training.pdf:application/pdf},
}

@misc{shinnSparsityRooflineUnderstanding2023,
	title = {The Sparsity Roofline: Understanding the Hardware Limits of Sparse Neural Networks},
	url = {http://arxiv.org/abs/2310.00496},
	doi = {10.48550/arXiv.2310.00496},
	shorttitle = {The Sparsity Roofline},
	abstract = {We introduce the Sparsity Roofline, a visual performance model for evaluating sparsity in neural networks. The Sparsity Roofline jointly models network accuracy, sparsity, and theoretical inference speedup. Our approach does not require implementing and benchmarking optimized kernels, and the theoretical speedup becomes equal to the actual speedup when the corresponding dense and sparse kernels are well-optimized. We achieve this through a novel analytical model for predicting sparse network performance, and validate the predicted speedup using several real-world computer vision architectures pruned across a range of sparsity patterns and degrees. We demonstrate the utility and ease-of-use of our model through two case studies: (1) we show how machine learning researchers can predict the performance of unimplemented or unoptimized block-structured sparsity patterns, and (2) we show how hardware designers can predict the performance implications of new sparsity patterns and sparse data formats in hardware. In both scenarios, the Sparsity Roofline helps performance experts identify sparsity regimes with the highest performance potential.},
	number = {{arXiv}:2310.00496},
	publisher = {{arXiv}},
	author = {Shinn, Cameron and {McCarthy}, Collin and Muralidharan, Saurav and Osama, Muhammad and Owens, John D.},
	urldate = {2025-12-18},
	date = {2023-11-06},
	eprinttype = {arxiv},
	eprint = {2310.00496 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/alxkp/Zotero/storage/6YHM5N2B/Shinn et al. - 2023 - The Sparsity Roofline Understanding the Hardware Limits of Sparse Neural Networks.pdf:application/pdf;Snapshot:/Users/alxkp/Zotero/storage/4XFWM2A5/2310.html:text/html},
}

@misc{liuImprovedBaselinesVisual2024,
	title = {Improved Baselines with Visual Instruction Tuning},
	url = {http://arxiv.org/abs/2310.03744},
	doi = {10.48550/arXiv.2310.03744},
	abstract = {Large multimodal models ({LMM}) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in {LLaVA} is surprisingly powerful and data-efficient. With simple modifications to {LLaVA}, namely, using {CLIP}-{ViT}-L-336px with an {MLP} projection and adding academic-task-oriented {VQA} data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in {\textasciitilde}1 day on a single 8-A100 node. We hope this can make state-of-the-art {LMM} research more accessible. Code and model will be publicly available.},
	number = {{arXiv}:2310.03744},
	publisher = {{arXiv}},
	author = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
	urldate = {2025-12-18},
	date = {2024-05-15},
	eprinttype = {arxiv},
	eprint = {2310.03744 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/alxkp/Zotero/storage/4J4SHA9L/Liu et al. - 2024 - Improved Baselines with Visual Instruction Tuning.pdf:application/pdf;Snapshot:/Users/alxkp/Zotero/storage/6NY985YY/2310.html:text/html},
}

@misc{liuVisualInstructionTuning2023,
	title = {Visual Instruction Tuning},
	url = {http://arxiv.org/abs/2304.08485},
	doi = {10.48550/arXiv.2304.08485},
	abstract = {Instruction tuning large language models ({LLMs}) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only {GPT}-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce {LLaVA}: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and {LLM} for general-purpose visual and language understanding.Our early experiments show that {LLaVA} demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal {GPT}-4 on unseen images/instructions, and yields a 85.1\% relative score compared with {GPT}-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science {QA}, the synergy of {LLaVA} and {GPT}-4 achieves a new state-of-the-art accuracy of 92.53\%. We make {GPT}-4 generated visual instruction tuning data, our model and code base publicly available.},
	number = {{arXiv}:2304.08485},
	publisher = {{arXiv}},
	author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
	urldate = {2025-12-18},
	date = {2023-12-11},
	eprinttype = {arxiv},
	eprint = {2304.08485 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/alxkp/Zotero/storage/8T4NPIKJ/Liu et al. - 2023 - Visual Instruction Tuning.pdf:application/pdf;Snapshot:/Users/alxkp/Zotero/storage/4JWD7YWA/2304.html:text/html},
}

@misc{wuReinforcingSpatialReasoning2025,
	title = {Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing},
	url = {http://arxiv.org/abs/2506.09965},
	doi = {10.48550/arXiv.2506.09965},
	abstract = {As textual reasoning with large language models ({LLMs}) has advanced significantly, there has been growing interest in enhancing the multimodal reasoning capabilities of large vision-language models ({LVLMs}). However, existing methods primarily approach multimodal reasoning in a straightforward, text-centric manner, where both reasoning and answer derivation are conducted purely through text, with the only difference being the presence of multimodal input. As a result, these methods often encounter fundamental limitations in spatial reasoning tasks that demand precise geometric understanding and continuous spatial tracking-capabilities that humans achieve through mental visualization and manipulation. To address the limitations, we propose drawing to reason in space, a novel paradigm that enables {LVLMs} to reason through elementary drawing operations in the visual space. By equipping models with basic drawing operations, including annotating bounding boxes and drawing auxiliary lines, we empower them to express and analyze spatial relationships through direct visual manipulation, meanwhile avoiding the performance ceiling imposed by specialized perception tools in previous tool-integrated reasoning approaches. To cultivate this capability, we develop a three-stage training framework: cold-start training with synthetic data to establish basic drawing abilities, reflective rejection sampling to enhance self-reflection behaviors, and reinforcement learning to directly optimize for target rewards. Extensive experiments demonstrate that our model, named {VILASR}, consistently outperforms existing methods across diverse spatial reasoning benchmarks, involving maze navigation, static spatial reasoning, video-based reasoning, and multi-view-based reasoning tasks, with an average improvement of 18.4\%.},
	number = {{arXiv}:2506.09965},
	publisher = {{arXiv}},
	author = {Wu, Junfei and Guan, Jian and Feng, Kaituo and Liu, Qiang and Wu, Shu and Wang, Liang and Wu, Wei and Tan, Tieniu},
	urldate = {2025-12-18},
	date = {2025-06-19},
	eprinttype = {arxiv},
	eprint = {2506.09965 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/alxkp/Zotero/storage/Y3FKK5I8/Wu et al. - 2025 - Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing.pdf:application/pdf},
}

@misc{liBLIP2BootstrappingLanguageImage2023,
	title = {{BLIP}-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
	url = {http://arxiv.org/abs/2301.12597},
	doi = {10.48550/arXiv.2301.12597},
	shorttitle = {{BLIP}-2},
	abstract = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes {BLIP}-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. {BLIP}-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. {BLIP}-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7\% on zero-shot {VQAv}2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
	number = {{arXiv}:2301.12597},
	publisher = {{arXiv}},
	author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
	urldate = {2025-12-18},
	date = {2023-06-15},
	eprinttype = {arxiv},
	eprint = {2301.12597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/alxkp/Zotero/storage/JMI9B6N9/Li et al. - 2023 - BLIP-2 Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Mode.pdf:application/pdf},
}
